{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import codecs\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (24, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(path):\n",
    "    x = pd.read_csv(path)\n",
    "    x.rename(columns={'Unnamed: 0': 'description'}, inplace=True)\n",
    "    x = x.set_index('description')\n",
    "    return x\n",
    "X = csv_to_df('csv_files/ModelFiles/photogallery_with_tag_for_xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lst = list(X.columns)\n",
    "lbls = np.array(X['User_Register'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.drop('User_Register', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_lbls, test_lbls = train_test_split(dat, lbls, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    #errors = abs(predictions - y_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "    #print('Model Performance')\n",
    "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    #print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    return accuracy, conf_matrix, class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 18.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 24.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_data, train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_leaf=4, min_samples_split=5,\n",
       "                       n_estimators=200)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "best_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.63%.\n"
     ]
    }
   ],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy, confusion_mat, classification_report = evaluate(best_random, test_data, test_lbls)\n",
    "print('Accuracy = {:0.2f}%.'.format(random_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169  12]\n",
      " [ 90   8]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "C[0,0] - true negative (169)\n",
    "C[1,0] - false negative (90)\n",
    "C[0,1] - false positive (12)\n",
    "C[1,1] - true positive (8)\n",
    "'''\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prcision = correct_identify / total_identify    \n",
    "      example for positive is :   true_positive / (true_positive + true_negative)\n",
    "# recall = correct_identify / total_class_member\n",
    "      example for positive is :   true_positive / (true_positive + false_negative)\n",
    "# f1-score = good classifier estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.93      0.77       181\n",
      "         1.0       0.40      0.08      0.14        98\n",
      "\n",
      "    accuracy                           0.63       279\n",
      "   macro avg       0.53      0.51      0.45       279\n",
      "weighted avg       0.56      0.63      0.55       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importence(rf, feature_list):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(rf.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Print out the feature and importances \n",
    "    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: mean number of photos in burst at night Importance: 0.05\n",
      "Variable: ar of num of photos in weekdays lag-1 Importance: 0.05\n",
      "Variable: mean number of photos in burst Importance: 0.04\n",
      "Variable: mean number of bursts a week Importance: 0.04\n",
      "Variable: entropy on number of photos taken in night time Importance: 0.04\n",
      "Variable: ar of num of photos in night time lag-1 Importance: 0.04\n",
      "Variable: mean number of bursts a week nights Importance: 0.04\n",
      "Variable: entropy on number of photos taken in day time Importance: 0.04\n",
      "Variable: ar of num of photos in day time lag-1 Importance: 0.04\n",
      "Variable: mean number of photos in burst at daytime Importance: 0.04\n",
      "Variable: mean number of bursts a week daytime Importance: 0.04\n",
      "Variable: sampled weekends ratio Importance: 0.04\n",
      "Variable: entropy on number of photos taken in weekend Importance: 0.04\n",
      "Variable: mean number of photos in burst at weekend Importance: 0.04\n",
      "Variable: mean number of bursts in weekend Importance: 0.04\n",
      "Variable: entropy on number of photos taken in weekdays Importance: 0.04\n",
      "Variable: mean number of photos in burst at weekdays Importance: 0.04\n",
      "Variable: mean number of bursts in weekdays Importance: 0.04\n",
      "Variable: sample weeks ratio   Importance: 0.03\n",
      "Variable: sampled week nights ratio Importance: 0.03\n",
      "Variable: sampled week days ratio Importance: 0.03\n",
      "Variable: ar of num of photos in weekend lag-1 Importance: 0.03\n",
      "Variable: sampled weekdays ratio Importance: 0.03\n",
      "Variable: median work days number photos Importance: 0.02\n",
      "Variable: median number photos taken in week - location Importance: 0.01\n",
      "Variable: mad number photos taken in week - scale Importance: 0.01\n",
      "Variable: median night time number photos Importance: 0.01\n",
      "Variable: mad night time number photos Importance: 0.01\n",
      "Variable: median day time number photos Importance: 0.01\n",
      "Variable: mad day time number photos Importance: 0.01\n",
      "Variable: median weekend number photos Importance: 0.01\n",
      "Variable: mad weekend number photos Importance: 0.01\n",
      "Variable: mad work days number photos Importance: 0.01\n"
     ]
    }
   ],
   "source": [
    "feature_importence(best_random, feature_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
